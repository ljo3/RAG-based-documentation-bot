{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "785e425b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f109d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved: FastEmbed is a high-performance embedding generation library by Qdrant. (Score: 0.6500)\n",
      "AI Answer: There’s no widely known project called “FastMbed” — you probably meant FastEmbed.  \n",
      "\n",
      "FastEmbed is Qdrant’s high-performance embedding-generation library. It’s designed to produce vector embeddings efficiently (high throughput, low latency), integrate with model backends, and fit into vector-search workflows (e.g., with Qdrant). If you meant something else by FastMbed, tell me where you saw it and I’ll look into it.\n"
     ]
    }
   ],
   "source": [
    "# pip install fastembed scikit-learn openai numpy\n",
    "from fastembed import TextEmbedding\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# SETUP\n",
    "client = OpenAI()\n",
    "documents = [\n",
    "    \"LlamaIndex is a framework for connecting data to LLMs.\",\n",
    "    \"FastEmbed is a high-performance embedding generation library by Qdrant.\",\n",
    "    \"Qdrant is a vector database written in Rust.\"\n",
    "]\n",
    "\n",
    "# 1. EMBED (Local & Free)\n",
    "# FastEmbed returns a generator, so we convert to list\n",
    "embed_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "doc_embeddings = list(embed_model.embed(documents)) # List of numpy arrays\n",
    "\n",
    "# 2. RETRIEVE (Manual Math with Sklearn)\n",
    "query = \"What is FastMbed?\"\n",
    "query_embedding = list(embed_model.embed([query]))[0]\n",
    "\n",
    "# Calculate similarity between Query and ALL documents\n",
    "# We must stack the list of arrays into a single matrix for sklearn\n",
    "scores = cosine_similarity([query_embedding], np.stack(doc_embeddings))[0]\n",
    "\n",
    "# Find the index of the highest score\n",
    "best_doc_index = np.argmax(scores)\n",
    "retrieved_doc = documents[best_doc_index]\n",
    "\n",
    "print(f\"Retrieved: {retrieved_doc} (Score: {scores[best_doc_index]:.4f})\")\n",
    "\n",
    "# 3. GENERATE (OpenAI)\n",
    "prompt = f\"Context: {retrieved_doc}\\nQuestion: {query}\\nAnswer:\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "print(\"AI Answer:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3432c3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex connects data to LLMs.\n"
     ]
    }
   ],
   "source": [
    "# pip install llama-index llama-index-llms-openai\n",
    "# import os\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "\n",
    "# SETUP\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "documents = [Document(text=\"LlamaIndex connects data to LLMs.\")]\n",
    "\n",
    "# 1. INDEX (Auto-Embeds via OpenAI API)\n",
    "# LlamaIndex handles the API calls, batching, and vector storage automatically.\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 2. RETRIEVE & GENERATE\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What does LlamaIndex do?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac5b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To save API costs.\n"
     ]
    }
   ],
   "source": [
    "# pip install llama-index llama-index-embeddings-fastembed\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "\n",
    "# --- THE MAGIC SWITCH ---\n",
    "# We globally configure LlamaIndex to use FastEmbed running locally on your CPU\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "documents = [\n",
    "    Document(text=\"FastEmbed runs locally and saves API costs.\"),\n",
    "    Document(text=\"LlamaIndex orchestrates the retrieval flow.\")\n",
    "]\n",
    "\n",
    "# 1. INDEX (Local & Free)\n",
    "# This no longer calls OpenAI. It runs on your laptop.\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 2. RETRIEVE & GENERATE\n",
    "# The retrieval uses local vectors; the final answer uses GPT-4\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Why use FastEmbed?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5fd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd1ba84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 16:39:53,036 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UV is an extremely fast Python package installer that is written in Rust.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import qdrant_client\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# 1. SETUP: LLM & Embeddings\n",
    "# We use OpenAI for generation (requires key) but FastEmbed for embeddings (free/local)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "Settings.llm = OpenAI(model=\"gpt-4\")\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# 2. CONNECT: Qdrant\n",
    "# Note: For local docker use host=\"localhost\". For in-memory (testing) use location=\":memory:\"\n",
    "client = qdrant_client.QdrantClient(location=\":memory:\") \n",
    "\n",
    "# 3. STORAGE: Configure LlamaIndex to use Qdrant\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"uv_demo\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 4. INDEX: Load, Embed (Locally), and Store (in Qdrant)\n",
    "# Create a dummy file if you don't have one\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "    with open(\"data/test.txt\", \"w\") as f:\n",
    "        f.write(\"uv is an extremely fast Python package installer written in Rust.\")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "# 5. QUERY\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is uv?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197a503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
